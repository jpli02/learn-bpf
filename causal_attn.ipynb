{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPK9LNXaXT1Fj+6MDTaXXbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpli02/learn-bpf/blob/master/causal_attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton==2.0.0.dev20220709\n",
        "!pip install pytest\n",
        "import pytest\n",
        "!pip install flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ial6_dZG-wo",
        "outputId": "23baeb77-bccf-4dbc-cc77-73f0caa6807b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement triton==2.0.0.dev20220709 (from versions: 2.0.0, 2.1.0, 2.2.0, 2.3.0, 2.3.1, 3.0.0, 3.1.0, 3.2.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for triton==2.0.0.dev20220709\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.5.0)\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m756.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kZvICdvSG7TB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "def is_hip():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _attn_fwd_inner(acc, l_i, m_i, q,  #\n",
        "                    K_block_ptr, V_block_ptr, #\n",
        "                    start_m, qk_scale,  #\n",
        "                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
        "                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr, #\n",
        "                    N_CTX: tl.constexpr, fp8_v: tl.constexpr):\n",
        "    # range of values handled by this stage\n",
        "    if STAGE == 1:\n",
        "        lo, hi = 0, start_m * BLOCK_M\n",
        "    elif STAGE == 2:\n",
        "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
        "        lo = tl.multiple_of(lo, BLOCK_M)\n",
        "    # causal = False\n",
        "    else:\n",
        "        lo, hi = 0, N_CTX\n",
        "\n",
        "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
        "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
        "    # loop over k, v and update accumulator\n",
        "    for start_n in range(lo, hi, BLOCK_N):\n",
        "        start_n = tl.multiple_of(start_n, BLOCK_N)\n",
        "        # -- compute qk ----\n",
        "        k = tl.load(K_block_ptr)\n",
        "        qk = tl.dot(q, k)\n",
        "        if STAGE == 2:\n",
        "            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n",
        "            qk = qk * qk_scale + tl.where(mask, 0, -1.0e6)\n",
        "            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n",
        "            qk -= m_ij[:, None]\n",
        "        else:\n",
        "            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n",
        "            qk = qk * qk_scale - m_ij[:, None]\n",
        "\n",
        "        p = tl.math.exp2(qk)\n",
        "        l_ij = tl.sum(p, 1)\n",
        "        # -- update m_i and l_i\n",
        "        alpha = tl.math.exp2(m_i - m_ij)\n",
        "        l_i = l_i * alpha + l_ij\n",
        "        # -- update output accumulator --\n",
        "        acc = acc * alpha[:, None]\n",
        "        # update acc\n",
        "        v = tl.load(V_block_ptr)\n",
        "        if fp8_v:\n",
        "            p = p.to(tl.float8e5)\n",
        "        else:\n",
        "            p = p.to(tl.float16)\n",
        "        acc = tl.dot(p, v, acc)\n",
        "        # update m_i and l_i\n",
        "        m_i = m_ij\n",
        "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n",
        "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n",
        "\n",
        "    return acc, l_i, m_i\n",
        "\n",
        "@triton.jit\n",
        "def _acc_attention_score(acc_score, k,  #\n",
        "                        Q_block_ptr, M_block_ptr, #\n",
        "                        start_m, qk_scale,  #\n",
        "                        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
        "                        STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr, #\n",
        "                        N_CTX: tl.constexpr):\n",
        "\n",
        "    # range of values handled by this stage\n",
        "    lo, hi = 0, N_CTX\n",
        "    for start_n in range(lo, hi, BLOCK_M):\n",
        "        q = tl.load(Q_block_ptr)\n",
        "        m = tl.load(M_block_ptr)\n",
        "        qk = tl.dot(q, k)\n",
        "        if STAGE == 1 or STAGE == 2:\n",
        "          mask = (offs_m[:, None] + start_n) >= (offs_n[None, :])\n",
        "          qk = qk * qk_scale + tl.where(mask, 0, -1.0e8) - m[:, None]\n",
        "        # causal = False\n",
        "        else:\n",
        "          qk = qk * qk_scale  - m[:, None]\n",
        "\n",
        "        p = tl.math.exp2(qk)\n",
        "\n",
        "        acc_score += tl.sum(p, 0)\n",
        "        Q_block_ptr = tl.advance(Q_block_ptr, (BLOCK_M, 0))\n",
        "        M_block_ptr = tl.advance(M_block_ptr, (BLOCK_M,))\n",
        "\n",
        "    return acc_score\n",
        "\n",
        "# We don't run auto-tuning every time to keep the tutorial fast. Keeping`\n",
        "# the code below and commenting out the equivalent parameters is convenient for\n",
        "# re-tuning.\n",
        "configs = [\n",
        "    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w) \\\n",
        "    # for BM in [64, 128]\\\n",
        "    # for BN in [32, 64]\\\n",
        "    for BM in [16]\\\n",
        "    for BN in [16]\\\n",
        "    for s in ([1] if is_hip() else [3, 4, 7])\\\n",
        "    for w in [4, 8]\\\n",
        "]\n",
        "\n",
        "\n",
        "def keep(conf):\n",
        "    BLOCK_M = conf.kwargs[\"BLOCK_M\"]\n",
        "    BLOCK_N = conf.kwargs[\"BLOCK_N\"]\n",
        "    if BLOCK_M * BLOCK_N < 128 * 128 and conf.num_warps == 8:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "@triton.autotune(list(filter(keep, configs)), key=[\"N_CTX\", \"HEAD_DIM\"])\n",
        "@triton.jit\n",
        "def _attn_fwd(Q, K, V, sm_scale, M, Out, C, # C = (Z, H, N_CTX)\n",
        "              stride_qz, stride_qh, stride_qm, stride_qk,  #\n",
        "              stride_kz, stride_kh, stride_kn, stride_kk,  #\n",
        "              stride_vz, stride_vh, stride_vk, stride_vn,  #\n",
        "              stride_oz, stride_oh, stride_om, stride_on,  #\n",
        "              stride_cz, stride_ch, stride_cn,  #\n",
        "              stride_mz, stride_mh, stride_mn, #\n",
        "              Z, H, N_CTX,  #\n",
        "              HEAD_DIM: tl.constexpr,  #\n",
        "              BLOCK_M: tl.constexpr,  #\n",
        "              BLOCK_N: tl.constexpr,  #\n",
        "              STAGE: tl.constexpr  #\n",
        "              ):\n",
        "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
        "    start_m = tl.program_id(0)\n",
        "    off_hz = tl.program_id(1)\n",
        "    off_z = off_hz // H\n",
        "    off_h = off_hz % H\n",
        "\n",
        "    # corresponds to a q, k and v for a particular head and batch\n",
        "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n",
        "\n",
        "    # block pointers\n",
        "    Q_block_ptr = tl.make_block_ptr(\n",
        "        base=Q + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_qm, stride_qk),\n",
        "        offsets=(start_m * BLOCK_M, 0),\n",
        "        block_shape=(BLOCK_M, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n",
        "    V_block_ptr = tl.make_block_ptr(\n",
        "        base=V + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_vk, stride_vn),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(BLOCK_N, HEAD_DIM),\n",
        "        order=v_order,\n",
        "    )\n",
        "    K_block_ptr = tl.make_block_ptr(\n",
        "        base=K + qvk_offset,\n",
        "        shape=(HEAD_DIM, N_CTX),\n",
        "        strides=(stride_kk, stride_kn),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(HEAD_DIM, BLOCK_N),\n",
        "        order=(0, 1),\n",
        "    )\n",
        "    O_block_ptr = tl.make_block_ptr(\n",
        "        base=Out + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_om, stride_on),\n",
        "        offsets=(start_m * BLOCK_M, 0),\n",
        "        block_shape=(BLOCK_M, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "\n",
        "    # initialize offsets\n",
        "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = tl.arange(0, BLOCK_N)\n",
        "\n",
        "    # initialize pointer to m and l\n",
        "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
        "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n",
        "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
        "\n",
        "    # load scales\n",
        "    qk_scale = sm_scale\n",
        "    qk_scale *= 1.44269504  # 1/log(2)\n",
        "    # load q: it will stay in SRAM throughout\n",
        "    q = tl.load(Q_block_ptr)\n",
        "    # stage 1: off-band\n",
        "    # For causal = True, STAGE = 3 and _attn_fwd_inner gets 1 as its STAGE\n",
        "    # For causal = False, STAGE = 1, and _attn_fwd_inner gets 3 as its STAGE\n",
        "    if STAGE & 1:\n",
        "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr,  #\n",
        "                                        start_m, qk_scale,  #\n",
        "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
        "                                        4 - STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5  #\n",
        "                                        )\n",
        "    # stage 2: on-band\n",
        "    if STAGE & 2:\n",
        "        # barrier makes it easier for compielr to schedule the\n",
        "        # two loops independently\n",
        "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr,  #\n",
        "                                        start_m, qk_scale,  #\n",
        "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
        "                                        2, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5  #\n",
        "                                        )\n",
        "    # epilogue\n",
        "    m_i += tl.math.log2(l_i)\n",
        "    acc = acc / l_i[:, None]\n",
        "    m_ptrs = M + off_hz * N_CTX + offs_m\n",
        "    tl.store(m_ptrs, m_i)\n",
        "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))\n",
        "\n",
        "    # second-pass accumulated score calculation\n",
        "    # required condition: BLOCK_M == BLOCK_N\n",
        "    m_offset = off_z.to(tl.int64) * stride_mz + off_h.to(tl.int64) * stride_mh\n",
        "    c_offset = off_z.to(tl.int64) * stride_cz + off_h.to(tl.int64) * stride_ch\n",
        "\n",
        "    Q_block_ptr = tl.make_block_ptr(\n",
        "        base=Q + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_qm, stride_qk),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(BLOCK_M, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    K_block_ptr = tl.make_block_ptr(\n",
        "        base=K + qvk_offset,\n",
        "        shape=(HEAD_DIM, N_CTX),\n",
        "        strides=(stride_kk, stride_kn),\n",
        "        offsets=(0, BLOCK_N * start_m),\n",
        "        block_shape=(HEAD_DIM, BLOCK_N),\n",
        "        order=(0, 1),\n",
        "    )\n",
        "    C_block_ptr = tl.make_block_ptr(\n",
        "        base=C + c_offset,\n",
        "        shape=(N_CTX,),\n",
        "        strides=(stride_cn,),\n",
        "        offsets=(start_m * BLOCK_N,),\n",
        "        block_shape=(BLOCK_N,),\n",
        "        order=(0,),\n",
        "    )\n",
        "\n",
        "    M_block_ptr = tl.make_block_ptr(\n",
        "        base=M + m_offset,\n",
        "        shape=(N_CTX,),\n",
        "        strides=(stride_mn,),\n",
        "        offsets=(0,),\n",
        "        block_shape=(BLOCK_N,),\n",
        "        order=(0,)\n",
        "    )\n",
        "\n",
        "    offs_m = tl.arange(0, BLOCK_M)\n",
        "    offs_n = start_m * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "\n",
        "    acc_score = tl.zeros([BLOCK_N,], dtype=tl.float32)\n",
        "    k = tl.load(K_block_ptr)\n",
        "    acc_score = _acc_attention_score(acc_score, k,\n",
        "                    Q_block_ptr, M_block_ptr, #\n",
        "                    start_m, qk_scale,  #\n",
        "                    BLOCK_M, BLOCK_N,  #\n",
        "                    4 - STAGE, offs_m, offs_n, #\n",
        "                    N_CTX)\n",
        "\n",
        "\n",
        "    tl.store(C_block_ptr, acc_score.to(C.type.element_ty))\n",
        "\n",
        "\n",
        "class _attention(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, q, k, v, causal, sm_scale):\n",
        "        # shape constraints\n",
        "        HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n",
        "        # when v is in float8_e5m2 it is transposed.\n",
        "        HEAD_DIM_V = v.shape[-1]\n",
        "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
        "        assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
        "        o = torch.empty_like(q)\n",
        "        c = torch.zeros((q.shape[0], q.shape[1], q.shape[2]), dtype=torch.float32, device=q.device)\n",
        "        stage = 3 if causal else 1\n",
        "        extra_kern_args = {}\n",
        "        # Tuning for AMD target\n",
        "        if is_hip():\n",
        "            waves_per_eu = 3 if HEAD_DIM_K <= 64 else 2\n",
        "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
        "\n",
        "        grid = lambda args: (triton.cdiv(q.shape[2], args[\"BLOCK_M\"]), q.shape[0] * q.shape[1], 1)\n",
        "        M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float16)\n",
        "\n",
        "        _attn_fwd[grid](\n",
        "            q, k, v, sm_scale, M, o, c, #\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n",
        "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n",
        "            c.stride(0), c.stride(1), c.stride(2),  #\n",
        "            M.stride(0), M.stride(1), M.stride(2),\n",
        "            q.shape[0], q.shape[1],  #\n",
        "            N_CTX=q.shape[2],  #\n",
        "            HEAD_DIM=HEAD_DIM_K,  #\n",
        "            STAGE=stage,  #\n",
        "            **extra_kern_args)\n",
        "\n",
        "        ctx.save_for_backward(q, k, v, o, M)\n",
        "        ctx.grid = grid\n",
        "        ctx.sm_scale = sm_scale\n",
        "        ctx.HEAD_DIM = HEAD_DIM_K\n",
        "        ctx.causal = causal\n",
        "        return o, c, M\n",
        "\n",
        "\n",
        "selection_attention = _attention.apply"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import math\n",
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
        "\n",
        "def gpu_cleanup():\n",
        "    \"\"\"\n",
        "    Function to clean up GPU memory.\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def create_tensors(Z, H, N_CTX, HEAD_DIM, dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    Create tensors for attention computation.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(int(time.time()))\n",
        "    q = torch.rand((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=\"cuda\")\n",
        "    k = torch.rand((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=\"cuda\")\n",
        "    v = torch.rand((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=\"cuda\")\n",
        "    return q, k, v\n",
        "\n",
        "def _make_causal_mask(\n",
        "    bsz: int, tgt_len: int, past_key_values_length: int, dtype: torch.dtype, device: torch.device):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
        "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "def ref_attention(Z, H, N_CTX, HEAD_DIM, causal, dtype=torch.float16):\n",
        "    q, k, v = create_tensors(Z, H, N_CTX, HEAD_DIM, dtype)\n",
        "    attn_weights = torch.matmul(q, k.transpose(2,3)) / math.sqrt(HEAD_DIM)\n",
        "\n",
        "    if causal:\n",
        "        attention_mask = _make_causal_mask(\n",
        "            bsz=Z,\n",
        "            tgt_len=N_CTX,\n",
        "            past_key_values_length=0,\n",
        "            dtype=q.dtype,\n",
        "            device=q.device,\n",
        "        )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "            attn_weights = torch.max(\n",
        "                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n",
        "            )\n",
        "\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float16).to(q.dtype)\n",
        "    attn_output = torch.matmul(attn_weights, v)\n",
        "    cumulative_attn_map = attn_weights.sum(2)\n",
        "    return attn_output, cumulative_attn_map\n",
        "\n",
        "def flash_attention(Z, H, N_CTX, HEAD_DIM, causal, dtype=torch.float16):\n",
        "    q, k, v = create_tensors(Z, H, N_CTX, HEAD_DIM, dtype)\n",
        "    attn_output = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=causal,\n",
        "                window_size=(-1, -1), alibi_slopes=None, deterministic=False)\n",
        "\n",
        "    return attn_output\n",
        "\n",
        "def triton_attention(Z, H, N_CTX, HEAD_DIM, causal, dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    Perform Triton-based attention computation on the GPU.\n",
        "    \"\"\"\n",
        "    q, k, v = create_tensors(Z, H, N_CTX, HEAD_DIM, dtype)\n",
        "    sm_scale = 1.0 / math.sqrt(HEAD_DIM)\n",
        "    tri_out, tri_c, tri_m = selection_attention(q, k, v, causal, sm_scale)\n",
        "    return tri_out, tri_c, tri_m\n",
        "\n",
        "def test_attention(Z, H, N_CTX, HEAD_DIM, causal=False, dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    Test to compare correctness of triton cmul attention kernel\n",
        "    \"\"\"\n",
        "    gpu_cleanup()\n",
        "    ref_out_gpu1, ref_c_gpu1 = ref_attention(Z, H, N_CTX, HEAD_DIM, causal, dtype)\n",
        "    # Convert reference tensors to match dtype of Triton results\n",
        "    ref_c_gpu1 = ref_c_gpu1.half()\n",
        "    ref_out_gpu1 = ref_out_gpu1.half()\n",
        "    tri_out_gpu, tri_c_gpu, tri_m_gpu = triton_attention(Z, H, N_CTX, HEAD_DIM, causal, dtype)\n",
        "\n",
        "    flash_out_gpu = flash_attention(Z, H, N_CTX, HEAD_DIM, causal, dtype)\n",
        "\n",
        "    # Compare results\n",
        "    print(f\"Attention max diff: {(tri_out_gpu.half() - ref_out_gpu1).abs().max().item()}\")\n",
        "    assert torch.allclose(ref_out_gpu1, tri_out_gpu.half(), atol=0.8, rtol=0), \"Attention output mismatch\"\n",
        "    print(\"Attention check passed\")\n",
        "\n",
        "    print(f\"accum score max diff: {(tri_c_gpu.half() - ref_c_gpu1).abs().max().item()}\")\n",
        "    # print(\"---------------------------------------------\")\n",
        "    # print(\"ref attention\")\n",
        "    # print(ref_c_gpu1)\n",
        "    # print(\"---------------------------------------------\")\n",
        "    # print(\"triton attention\")\n",
        "    # print(tri_c_gpu.half())\n",
        "\n",
        "    assert torch.allclose(ref_c_gpu1, tri_c_gpu.half(), atol=0.05, rtol=0), \"col-wise sum score acc mismatch\"\n",
        "    print(\"Attention score acc check passed\")\n",
        "\n",
        "    # save results\n",
        "    # pd.DataFrame(ref_c_gpu.cpu().numpy().flatten()).to_csv(\"/u/ndani/selection_kernel/reference_scores.csv\", index=False, header=False, float_format=\"%.5f\")\n",
        "    # pd.DataFrame(tri_c_gpu.cpu().numpy().flatten()).to_csv(\"/u/ndani/selection_kernel/ours_scores.csv\", index=False, header=False, float_format=\"%.5f\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Execute the test\n",
        "    # test_attention(16, 32, 4096, 16, False)\n",
        "    print(\"causal false passed\")\n",
        "\n",
        "    test_attention(16, 32, 1024, 16, True)"
      ],
      "metadata": {
        "id": "87O5yQRDIIBd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "3f853109-aa3a-4fc0-95ee-08fd2c8f9816"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "causal false passed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "FlashAttention only supports Ampere GPUs or newer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-29467a154764>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"causal false passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mtest_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-29467a154764>\u001b[0m in \u001b[0;36mtest_attention\u001b[0;34m(Z, H, N_CTX, HEAD_DIM, causal, dtype)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mtri_out_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtri_c_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtri_m_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriton_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CTX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEAD_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mflash_out_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflash_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CTX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEAD_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Compare results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-29467a154764>\u001b[0m in \u001b[0;36mflash_attention\u001b[0;34m(Z, H, N_CTX, HEAD_DIM, causal, dtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mflash_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CTX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEAD_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CTX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEAD_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     attn_output = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=causal,\n\u001b[0m\u001b[1;32m     68\u001b[0m                 window_size=(-1, -1), alibi_slopes=None, deterministic=False)\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m in \u001b[0;36mflash_attn_func\u001b[0;34m(q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs)\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0mpattern\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mdropped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonnegative\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mkept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \"\"\"\n\u001b[0;32m-> 1201\u001b[0;31m     return FlashAttnFunc.apply(\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_softmax, is_grad_enabled)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhead_size_og\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhead_size_og\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_forward(\n\u001b[0m\u001b[1;32m    840\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/autograd.py\u001b[0m in \u001b[0;36mautograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword_only_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword_only_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/autograd.py\u001b[0m in \u001b[0;36mforward_no_grad\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mkeyset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyword_only_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyset\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_autograd_keyset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mredispatch\u001b[0;34m(self, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mredispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredispatch_boxed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36mbackend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mbackend_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mget_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m in \u001b[0;36m_flash_attn_forward\u001b[0;34m(q, k, v, dropout_p, softmax_scale, causal, window_size_left, window_size_right, softcap, alibi_slopes, return_softmax)\u001b[0m\n\u001b[1;32m     94\u001b[0m ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\u001b[1;32m     95\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmaybe_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: FlashAttention only supports Ampere GPUs or newer."
          ]
        }
      ]
    }
  ]
}